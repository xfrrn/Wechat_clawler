import requests,json,os,re
from bs4 import BeautifulSoup
from storage import persist_session
from typing import Dict
COKKIES_PATH = os.path.join("cfg","cookies.json")
Session = requests.Session()

def get_fakeid_by_name(wx_cfg, kw):
  """
  æ ¹æ®å…¬ä¼—å·åç§°å…³é”®è¯è·å–å…¬ä¼—å·çš„ fakeidã€‚
  
  Args:
    wx_cfg (dict): åŒ…å« tokenã€cookies_strã€user_agent ç­‰è®¤è¯ä¿¡æ¯çš„é…ç½®å­—å…¸ã€‚
    kw (str): å…¬ä¼—å·åç§°å…³é”®è¯ã€‚
  
  Returns:
    str or None: è·å–åˆ°çš„ fakeidï¼Œæœªè·å–åˆ°åˆ™è¿”å› Noneã€‚
  """
  url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
  params = {
    "action": "search_biz",
    "begin": 0,
    "count": 5,
    "query": kw,
    "token": wx_cfg.get("token"),
    "lang": "zh_CN",
    "f": "json",
    "ajax": "1"
  }
  headers = {
    "Cookie": wx_cfg.get("cookies_str"),
    "User-Agent": wx_cfg.get("user_agent")
  }
  Session.headers.update(headers)
  resp = Session.get(url, params=params)
  print("çŠ¶æ€ç :", resp.status_code)
  print("å“åº”å†…å®¹:", resp.text)
  # ä¿ç•™ searchbiz è¿”å›å†…å®¹
  try:
    data = resp.json()
  except Exception:
    data = resp.text
  with open("searchbiz_result.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
  try:
    fakeid = data["list"][0]["fakeid"]
    print(f"[ä¿¡æ¯] è·å–åˆ° fakeid: {fakeid}")
    return fakeid
  except Exception:
    print("[é”™è¯¯] æœªè·å–åˆ° fakeid")
    return None

def get_article_list(wx_cfg, fakeid, count=5):
  """
  è·å–æŒ‡å®š fakeid å…¬ä¼—å·çš„å†å²æ–‡ç« åˆ—è¡¨ã€‚
  
  Args:
    wx_cfg (dict): åŒ…å« tokenã€cookies_strã€user_agent ç­‰è®¤è¯ä¿¡æ¯çš„é…ç½®å­—å…¸ã€‚
    fakeid (str): å…¬ä¼—å·çš„ fakeidã€‚
    count (int, optional): è·å–çš„æ–‡ç« æ•°é‡ï¼Œé»˜è®¤ä¸º 5ã€‚
  
  Returns:
    None: ç»“æœé€šè¿‡ persist_session æŒä¹…åŒ–ä¿å­˜ã€‚
  """
  url = "https://mp.weixin.qq.com/cgi-bin/appmsgpublish"
  params = {
    "sub": "list",
    "sub_action": "list_ex",
    "begin": 0,
    "count": count,
    "fakeid": fakeid,
    "token": wx_cfg.get("token"),
    "lang": "zh_CN",
    "f": "json",
    "ajax": 1
  }
  headers = {
    "Cookie": wx_cfg.get("cookies_str"),
    "User-Agent": wx_cfg.get("user_agent")
  }
  Session.headers.update(headers)
  resp = Session.get(url, params=params, headers=headers)
  print("çŠ¶æ€ç :", resp.status_code)
  print("å“åº”å†…å®¹:", resp.text)
  # ä¿ç•™ appmsgpublish è¿”å›å†…å®¹
  try:
    data = resp.json()
  except Exception:
    data = resp.text
  persist_session(data,"appmsgpublish_result.json")

def extract_title_url(input_file="appmsgpublish_result.json", output_file="title_url_map.json"):
  """
  ä» appmsgpublish_result.json æå–æ–‡ç« æ ‡é¢˜ä¸ URL çš„æ˜ å°„ï¼Œå¹¶ä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚
  
  Args:
    input_file (str): è¾“å…¥çš„ appmsgpublish ç»“æœ JSON æ–‡ä»¶è·¯å¾„ã€‚
    output_file (str): è¾“å‡ºçš„æ ‡é¢˜-URL æ˜ å°„ JSON æ–‡ä»¶è·¯å¾„ã€‚
  
  Returns:
    None: ç»“æœå†™å…¥ output_fileã€‚
  """
  import json
  import ast

  result = {}
  with open(input_file, "r", encoding="utf-8") as f:
    data = json.load(f)

  if isinstance(data, dict):
    data_list = [data]
  else:
    data_list = data
  
  for entry in data_list:
    # publish_page æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
    publish_page = entry.get("publish_page")
    if not publish_page:
      continue
    try:
      page_obj = json.loads(publish_page)
    except Exception:
      continue
    for pub in page_obj.get("publish_list", []):
      # publish_info ä¹Ÿæ˜¯å­—ç¬¦ä¸²
      try:
        info_obj = json.loads(pub.get("publish_info", "{}"))
      except Exception:
        continue
      for appmsg in info_obj.get("appmsgex", []):
        title = appmsg.get("title")
        link = appmsg.get("link")
        if title and link:
          # å¤„ç†åæ–œæ 
          link = link.replace("\\/", "/").replace("\\\\/", "/")
          result[title] = link
  # ä¿å­˜ç»“æœ
  with open(output_file, "w", encoding="utf-8") as f:
    json.dump(result, f, ensure_ascii=False, indent=2)
  print(f"[ä¿¡æ¯] å·²ä¿å­˜ title->URL åˆ° {output_file}")

def fetch_article_details(url,timeout) -> Dict:
  """
  è·å–å…¬ä¼—å·æ–‡ç« è¯¦æƒ…ï¼ŒåŒ…æ‹¬ä½œè€…ã€æ ‡é¢˜ã€å†…å®¹ç­‰ã€‚
  
  Args:
    url (str): å…¬ä¼—å·æ–‡ç« çš„ URLã€‚
    timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
  
  Returns:
    dict: åŒ…å« statusã€contentã€titleã€authorã€create_timeã€biz ç­‰ä¿¡æ¯ã€‚
      - status=1 è¡¨ç¤ºæˆåŠŸï¼Œ0 è¡¨ç¤ºå¤±è´¥ã€‚
  """
  headers = {
    "Referer": "https://mp.weixin.qq.com/",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
  }
  url = url.strip() #å»é™¤å¤´å°¾ç©ºæ ¼æ¢è¡Œ
  Session.headers.update(headers)
  print("-----å¼€å§‹è¯·æ±‚-----")
  resp = Session.get(url,timeout=timeout)
  if resp.status_code == 200:
    print("âˆšâˆšâˆšâˆšâˆšè¯·æ±‚æˆåŠŸï¼âˆšâˆšâˆšâˆšâˆš")
  else:
     print("Ã—Ã—Ã—Ã—Ã—è¯·æ±‚å¤±è´¥ğŸ¥¹Ã—Ã—Ã—Ã—Ã—")
     return {"status":0}
  resp.encoding = resp.apparent_encoding
  status = re.search("å½“å‰ç¯å¢ƒå¼‚å¸¸ï¼Œå®ŒæˆéªŒè¯åå³å¯ç»§ç»­è®¿é—®",resp.text)
  if status:
    print("!!!!!ç¯å¢ƒå¼‚å¸¸,ç¨‹åºæ‰§è¡Œå¤±è´¥!!!!!!")
    return {}
  html = resp.text
  soup = BeautifulSoup(html,"lxml")
  print("-----å¼€å§‹æœç´¢å…ƒç´ -----")
  content = soup.find("div",class_ = "rich_media_content").get_text("\n",strip=True)
  title = soup.find("h1",{"class":"rich_media_title","id":"activity-name"}).get_text(strip=True)
  author = soup.find("a",{"id":"js_name"}).get_text(strip=True)
  biz = re.search(r'var biz\s*=\s*"(.*?)";',html).group(1).replace('" || "','').replace('"','')
  if(biz):
    print(f"æ‰¾åˆ°å…¬ä¼—å·{author}fakeid:{biz}ï¼Œä¿å­˜åå¯ä»¥ç”¨äºè·å–æ–‡ç« åˆ—è¡¨")
  else:
    biz = ""
    print("æŸ¥æ‰¾å…¬ä¼—å·å¤±è´¥")
  create_time = re.search(r"var createTime = '(.*?)';",html).group(1)
  os.makedirs("HTML",exist_ok=True)
  os.makedirs("TEXT",exist_ok=True)
  os.makedirs("DocJson",exist_ok=True)
  file_name = re.sub(r'[\\/:*?"<>|]', "_", f"{author}-{title}-{create_time}.html")
  html_path = os.path.join("HTML",file_name)
  print(f"-----ä¿å­˜HTMLæºç åˆ°{os.path.abspath(html_path)}-----")
  with open(html_path,"w",encoding="utf-8") as f:
    f.write(html)
  file_name = re.sub(r'[\\/:*?"<>|]', "_", f"{author}-{title}-{create_time}.txt")
  text_path = os.path.join("TEXT",file_name)
  print(f"-----ä¿å­˜æ–‡ç« æ–‡æœ¬åˆ°{os.path.abspath(text_path)}-----")
  with open(text_path,"w",encoding="utf-8") as f:
    f.write(content)
  file_name = f"{author} {title}"
  json_path = os.path.join("DocJson",file_name)
  data = {
    "status":1,
    "content":content,
    "title":title,
    "author":author,
    "create_time":create_time,
    "biz":biz
  }
  persist_session(data,json_path)
  return data
